{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407ef5f9-ebff-43f3-9eef-3a5e139450ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8623cd-9cf9-4f35-aed9-c265086b937e",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e7c63-183e-4caa-8e7a-f7f47177cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV is a function in machine learning that tries all the combinations of the values passed in the dictionary and evaluates the\n",
    "model for each combination using the Cross-Validation method. It allows you to apply a grid search to an array of hyper-parameters and \n",
    "cross-validate your model using k-fold cross-validation. Grid search CV works by first specifying a grid of thetas to search over. \n",
    "For each theta, we perform Kfold CV with the parameter of our model set to theta. This gives a cv loss value for each theta and so \n",
    "we can pick the theta which minimizes cv loss.\n",
    "\n",
    "In other words, GridSearchCV is a way of systematically working through multiple combinations of parameter tunes, cross-validating as \n",
    "it goes to determine which tune gives the best performance. It is often used in machine learning to find the best hyperparameters for a modeL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e6a51f-09ae-4555-aebb-bb06c9998438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c6a21a-86fe-4237-9946-c5f1e0c1e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose\n",
    "one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b968f2b8-e189-4457-91b3-c58e42f7c7cd",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f0f3e-88b3-4239-87a4-5ecd117426b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Both GridSearchCV and RandomizedSearchCV are methods for tuning the parameters of a model to increase its generalizability.\n",
    "The main difference between the two is that in GridSearchCV, we try every combination of a preset list of values of the hyper-parameters \n",
    "and choose the best combination based on the cross-validation score. In RandomizedSearchCV, the model selects the combinations randomly \n",
    "from a range of values, and we have to define the number of iterations.\n",
    "\n",
    "GridSearchCV is useful when we have a small number of hyperparameters to tune. It is also useful when we have some prior knowledge about\n",
    "the hyperparameters and their values. On the other hand, RandomizedSearchCV is useful when we have a large number of hyperparameters to tune.\n",
    "It is also useful when we do not have any prior knowledge about the hyperparameters and their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa36fa9-9b52-491b-aa36-530af6872f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0671dee-d536-479e-a6a6-7f2c38608c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c3aaff-efd7-4fd9-a9ef-9cc2248a7d42",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed2048e-dd3d-4bb3-b8e7-2e3abcf911f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data leakage is a term used in machine learning to describe the situation in which the data used to train a model contains information \n",
    "about the target variable that would not be available in the real world. This can happen when the true label is included as a feature,\n",
    "or when the training and test data sets are not properly separated. Data leakage can cause the model to perform poorly or unrealistically \n",
    "on new data, since it relies on unfair information.\n",
    "\n",
    "For example, let’s say you’re building a model to predict whether a customer will buy a product or not. If you include the \n",
    "customer’s purchase history as a feature in your model, you’re likely to get very high accuracy on your training set.\n",
    "However, this feature is not available at prediction time and including it in your model will lead to data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124586c-cc4d-400e-9fea-dd58477b4a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256c70f9-718b-47ca-8d24-35835013a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5619ff4-7c64-4c93-80a0-c10748c01301",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c13d3-249d-4652-9500-d7b61bff30ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensuring data leakage is prevented when using machine learning:\n",
    "\n",
    "1.Ensure your data is secure and encrypted.\n",
    "2.Implement a strong data governance policy.\n",
    "3.Set up user authentication protocols.\n",
    "4.Audit and monitor data access.\n",
    "5.Train employees to recognize potential data leakage risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a82247-3dfa-497b-8d7d-14db786090eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de978300-8143-47e6-b6fb-26dbe0b96e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db654b01-cc4f-4a1c-ba17-c2e83c51682b",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bcd91f-7ae6-4304-8680-611e83cb618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. \n",
    "The matrix compares the actual target values with those predicted by the machine learning model. A good model is one which has high TP\n",
    "(True Positive) and TN (True Negative) rates, while low FP (False Positive) and FN (False Negative) rates.\n",
    "\n",
    "The confusion matrix is a class-wise distribution of the predictive performance of a classification model. \n",
    "It is an organized way of mapping the predictions to the original classes to which the data belong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8bdf2-8944-47c6-97a3-a781afa49737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d1a28c-5d84-4b30-80d2-2f095c5d0435",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ebb94f-c975-4337-bb83-3fbb72880d03",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb68db-2141-4ea0-a93b-884404786c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the context of a confusion matrix, precision is the ratio of true positives (TP) to the total number of predicted positives (TP + FP).\n",
    "Recall is the ratio of true positives to the total number of actual positives (TP + FN).\n",
    "\n",
    "Precision is a measure of how many of the predicted positive cases are actually positive. Recall is a measure of how many actual positive\n",
    "cases were correctly predicted as positive. Precision and recall are two metrics derived from the confusion matrix.\n",
    "\n",
    "In other words, precision measures how many of the predicted positive cases are actually positive, while recall measures how many \n",
    "actual positive cases were correctly predicted as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39017659-2715-4a32-bde8-866d05e8b5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da32bfbb-fb9a-4c45-b8df-1d2d3d97a804",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e074cba-81e4-4d84-a347-6a4de23841df",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf25cec5-bcd6-450c-95ce-e4e58f30d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A confusion matrix is a table used in classification problems to assess where errors in the model were made. The rows represent the actual \n",
    "classes the outcomes should have been, while the columns represent the predictions made by the model. It shows how many predictions\n",
    "are correct and incorrect per class, and helps in understanding the classes that are being confused by the model as other class.\n",
    "The confusion matrix gives insight into the errors being made by the classifier and the types of errors that are being made1.\n",
    "\n",
    "The confusion matrix can be used to calculate various metrics such as accuracy, precision, recall, F1 score etc. which can help in \n",
    "evaluating the performance of a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe153e-a901-4032-9ab5-508cea494227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19b3db6-9f71-4324-9674-c65a44225677",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they\n",
    "calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d30f5e-8ab0-47c2-a30f-6d8608fd2b3b",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd3ed4c-79b8-4ce6-b42f-6d2e8dd76e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Confusion metrics are performance metrics for classification machine learning models. They are derived from the confusion matrix, \n",
    "which is a summary of the correct and incorrect predictions made by a model. The confusion matrix uses the values of True Positive, \n",
    "False Positive, True Negative, and False Negative to measure the model’s performance. Some of the common confusion metrics are accuracy, \n",
    "precision, recall, and F1 score.\n",
    "\n",
    "Accuracy is the ratio of all correct predictions to all predictions. Precision is the ratio of correct positive predictions to all \n",
    "positive predictions. Recall is the ratio of correct positive predictions to all actual positive cases. F1 score is the harmonic mean \n",
    "of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6b976-b4ec-416c-80db-6895267e6dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201d03f2-ff70-4ec4-9502-1916e09ef16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73475fbb-a836-4d3f-8f58-634cfb594d4a",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8475458-0283-48e1-a23a-9cb0d95a1b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "The confusion matrix is a table that is used to evaluate the performance of a classification model. It shows the number of correct \n",
    "and incorrect predictions made by the classification model compared to the actual outcomes (target value) in the data. The accuracy of\n",
    "a model is the ratio of total correct instances to the total instances. The relationship between accuracy and confusion matrix is that\n",
    "accuracy is one of the metrics that can be derived from the confusion matrix. Other metrics that can be derived from the confusion matrix\n",
    "include precision, recall, and F1 score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fb6d72-19d6-40a3-a6a4-4da1380dd6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce2df49-f2f8-478a-9919-434d16ee3c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning\n",
    "model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12155d7-db09-4872-a4cd-19cc64d06142",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a4bec2-58db-4b44-8c37-51aebb9b87d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A confusion matrix is an N X N matrix that is used to evaluate the performance of a classification model. It compares the actual target \n",
    "values against the ones predicted by the ML model, providing a holistic view of how a classification model will work and the errors it \n",
    "will face. The confusion matrix shows the ways in which your classification model is confused when it makes predictions, giving insight\n",
    "not only into the errors being made by your classifier but more importantly the types of errors that are being made.\n",
    "\n",
    "The confusion matrix can be used to identify potential biases or limitations in your machine learning model. For example, if you have\n",
    "a dataset that is imbalanced (i.e., one class has many more examples than another), then your model may be biased towards predicting \n",
    "the majority class. In this case, you can use the confusion matrix to see how well your model is doing on each class and identify where it may be making mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c90691-ba8e-4724-a86b-3996f319164d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2cedf7-b13e-40b4-9032-95d00bcc3f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e27ae3-c8ad-4105-9235-cd499ea5d643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942ca9e7-8da7-4b1b-8b0f-c88e64f70f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
